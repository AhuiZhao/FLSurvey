2019-one shot-One-Shot Federated Learning

CMU的工作
Key-idea：
一次性FL，最佳top k的候选模型集合， 模型蒸馏的可行性探索（还不够完备）


摘要：
我们提出了一次性联合学习，其中中央服务器在单轮通信中通过联合设备网络学习全局模型。 我们采用集合学习和知识聚合的方法 - 使AUC相对于本地基线的平均相对增益达到51.5％，并达到（无法实现的）全球理想的90.1％。 我们讨论这些方法并确定未来工作的几个有希望的方向。

在两个领域探寻了可能性：
（1）传统的监督学习，其中每个设备生成其自己的一组具有相关标签的训练点，以及（2）半监督学习，其中除了受监督的设备数据之外，中央服务器还可以访问一些相关的未标记数据。


深入研究的点：
各个终端一次性完成训练，而不是像传统FL的增量更新  

在分类问题上面探寻了 监督学习和半监督学习的数据

监督学习的设置————合成模型：
由于本地模型会有很大的差异，所以最佳的候选模型集合（待合并的模型集合）可能仅包含少数几个来源于某些设备的本地模型，而不是全集,在监督学习上使用以下策略，来把他们挑出来：
交叉验证（Cross-Validation） 服务器提前规定baseline（在同一份测试数据上面算指标等），然后选择baseline最好的k个本地模型构成集合
Data Selection 服务器选择K个数据集量最大的模型
Random Selection 随机选择

构建候选集合Fk后，采用的是averaging求平均的方法得出最终模型

半监督的设置————蒸馏：
当设备数量很多时候，对每个Fk执行推断，开销很大很难进行
因此引入模型蒸馏，前提是服务器可以访问到无标记的公开数据集（因此是半监督假设）
在这个无标记的半监督数据上，通过模型蒸馏，利用分类最终的概率结果来拟合出更小的模型
他们提供了一种二分类的模型蒸馏拟合方法
用以应付K太大的时候

他们的实验是仅仅对比了
1、最佳k集合的方法 CV
2、采用半监督数据的 模型蒸馏方法
实验证明，采用半监督数据的模型蒸馏方法，在准确率上可以趋近与 最佳K集合的方法


结论：
我们的工作是对一次性联合学习的初步调查。 我们的实验表明，在联合设置中未开发的集合方法和蒸馏 - 产生了有希望的结果，并提出了有趣的未来方向。 这些包括：（1）识别具有相似本地数据分布的设备的“群组”（例如来自相同地理区域的设备），这将允许我们学习我们可以为每个设备个性化的集合，（2）探索正式的隐私保证 联邦环境中的蒸馏[6]，（3）通过从单次注射到单次联合学习来提高准确性，以及（4）在非凸模型（例如，深度神经网络）的背景下探索我们的方法。


