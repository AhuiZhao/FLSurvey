2019-non-iid-Robust and Communication-Efficient Federated Learning from Non-IID Data

key idea ：
稀疏三元压缩技术STC，而且在non-iid上也比FedAVG效果好
主要原因就是FedAVG在non-iid上 效果太烂了


摘要
联合学习允许多方联合训练他们的组合数据的深度学习模型，而没有任何参与者必须将他们的本地数据透露给中央服务器。然而，这种形式的保护隐私的协作学习是以训练期间显着的通信开销为代价的。为了解决这个问题，已经在分布式训练文献中提出了几种压缩方法，这些方法可以将所需通信量减少多达三个数量级。然而，这些现有方法在联合学习设置中仅具有有限的实用性，因为它们或者仅压缩从客户端到服务器的上游通信（使下游通信保持未压缩）或者仅在理想化条件下（例如客户端的iid分发）良好地执行。数据，通常在联合学习中找不到。在这项工作中，我们提出稀疏三元压缩（STC），这是一种新的压缩框架，专门用于满足联邦学习环境的要求。 STC通过一种新颖的机制扩展了现有的top-k梯度稀疏化压缩技术，以实现下游压缩以及重量更新的三元化和最佳Golomb编码。我们对四种不同学习任务的实验表明，STC在常见的联合学习方案中明显优于联合平均，其中客户a）持有非iid数据，b）在培训期间使用小批量，或c）客户数量大且每次沟通的参与率都很低。我们进一步表明，即使客户端持有iid数据并使用中等大小的批次进行培训，STC仍然表现出优于联合平均的意义，因为它在较少的训练迭代和较小的通信预算中实现了我们基准的固定目标精度。 。这些结果提倡联邦优化向高频低位宽通信的范式转变，特别是在带宽受限的学习环境中。



核心：为了解决通信开销

对比现有工作：
1、主要是解决从客户端到服务器的上游通信，没有考虑下游通信
2、仅考虑了理想的iid情况，实际上在应用场景中很难找到iid

提出了STC 稀疏三元压缩

STC通过新颖的机制扩展了 现有的top k梯度稀疏化压缩技术

以实现了下游压缩以及重要更新的三元化和最佳Goloma编码

在四种不同的学习任务中表明，在非iid的数据上，STC的效果明显优于联合平均



