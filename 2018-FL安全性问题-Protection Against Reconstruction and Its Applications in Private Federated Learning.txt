
Protection Against Reconstruction and Its Applications in Private Federated Learning


Abhishek Bhowmick1, John Duchi1,2, Julien Freudiger1, Gaurav Kapoor1, and Ryan Rogers1

研究传输隐私安全的   差分隐私相关
在这项工作中，我们将本地隐私保护与中央差异隐私结合起来，提出了一种私人进行模型培训的实用方法
对于性能损耗较少


联合学习已成为用户数据研究和实际训练模型的一个令人兴奋的方向。虽然数据在联合学习中仍然是分散的，但通常假设模型更新以明文形式从设备发送到服务器。差异隐私已被提议作为确保模型保持私有的一种方式，但这并未解决在服务器上可以看到模型更新并导致用户数据泄漏的问题。本地差异隐私是最强大的隐私保护形式之一，因此每个人的数据都是私有化的。然而，传统上使用的局部差异隐私可能被证明在许多高维问题中的隐私条件过于严格，例如在分布式模型拟合中。我们通过提供针对某些对手的保护来提出一种新的本地差异隐私范例。具体而言，我们确保具有有限先验信息的对手无法以极高的概率在原始数据内重建某些规定的容差。这种解释允许我们考虑更大的隐私参数。然后，我们在这个大型隐私参数体系中设计（最佳）DP机制。在这项工作中，我们将本地隐私保护与中央差异隐私结合起来，提出了一种私人进行模型培训的实用方法。此外，我们表明这些隐私限制在图像分类和语言模型中保持实用性，与没有这些隐私限制的联合学习相当。

