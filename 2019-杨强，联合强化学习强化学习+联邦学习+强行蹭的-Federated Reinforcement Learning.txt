2019-Federated Reinforcement Learning

杨强+微众+中山大学等人工作

key idea：

强化学习+联邦学习+强行蹭的

摘要：
在强化学习中，当状态的特征空间较小且训练数据有限时，建立高质量的police具有挑战性。 由于数据和模型的隐私要求，将数据或知识从代理直接传输到另一个代理将不起作用。 在本文中，我们提出了一种新的加强学习方法，在其他代理人的帮助下，即联邦强化学习（FRL），考虑隐私要求并为每个代理建立Q网络。 为了保护数据和模型的隐私，我们在更新本地模型时利用彼此共享信息的高斯微分。 在实验中，我们通过与各种基线进行比较，在两个不同的领域（Grid-world和Text2Action域）中评估我们的FRL框架。


强化学习+FL 为每个client 建立Q网络，在两个不同的领域中评估我们的FRL框架，Grid-world 和 Text2Action域

为了保护数据和模型的隐私，在更新本地模型时，利用彼此共享信息的高斯微分

关键点：
联邦强化学习（FRL），通过在各个client间共享有限的信息（Q网络的输出）来学习每个client的私有Q网络策略，在向其他人发送信息时”加密“并”解密“从别人那里收集到的信息时，我们假设这些client可能有与状态和行为相对应的奖励，而其他client只观察到状态和行动，但是奖励表明这些client无法自己制定决策规则，所有client都可以加入联盟来制定决策。

FRL假设 各个client不会直接分享他们的观测结果，
1、工作不同于传统的多设备分布式的强化学习————涉及全局状态的观测和个体动作与团体奖励等
2、也不同于强化学习中的迁移学习————就是学习中获得的经验转移到帮助提高相关但是不同的任务的成绩

FRL假设各州无法在代理商之间共享。
我们的FRL框架分三个阶段运作。最初，每个代理从其他代理收集Q网络的输出值，这些代理用Gausian差分“加密”。此外，它构建神经网络，例如MLP（多层感知器），以计算具有其自己的Q网络输出和加密值作为输入的全局Q网络输出。最后，它根据全球Q网络输出更新MLP及其自己的Q网络。请注意，MLP在代理之间共享，而代理自己的Q网络是其他人不知道的，不应根据训练过程中共享的加密Q网络输出推断出来。

FRL假设状态无法在代理间共享
三个阶段：
1、每个client收集来源于其他client的Q网络的输出，这个输出是用高斯差分加密的
2、建立了MLP神经网络来计算全局Q网络的输出，用自己的Q网络输出和其他人加密的输出来作为计算全局Q网络的输入
3、然后用全局Q网络的输出来更新MLP和其自己的Q网络

注意的是MLP是可以共享的，但是每个节点自己的Q网络仅独占，而且自己Q网络的输出也是加密共享（高斯差分加密）


Qπ(s, a) 指当前状态**s**在策略π下采取动作 a 的长期回报。


传统的机器学习旨在从大型数据集中训练模型，假设它们可从数据中心访问。然而，在许多实际应用中，来自客户端的数据集通常对隐私敏感，并且这样的数据中心通常难以保证构建高质量的模型。为了解决这个问题，Konecny等人。建议采用新的学习环境，即学习型学习，其目标是培训分类或聚类模型，培训数据包括分布在大量客户身上的文本，图像或视频（Konecny等，2016; McMahan等，2017）。与以前的联邦学习不同，我们提出了一种基于强化学习的新型联邦学习框架（Sutton＆Barto，1998; Mnih等，2015; Co-Reyes等，2018），即联邦强化学习（FRL），旨在通过在代理之间共享有限信息（即Q网络的输出）来学习每个代理的私有Q网络策略，在向其他人发送信息时“加密”并“解密”从别人那里收到信息时。我们假设某些代理人可能有与状态和行为相对应的奖励，而其他代理人只观察到状态和行动，但奖励表明这些代理人无法自己制定决策政策。我们声称所有代理商都可以通过加入联盟来制定决策策略。
关于联邦强化学习有很多应用。例如，在制造业中，生产产品可能涉及生产产品的不同组分的各种工厂。工厂的决策政策是私人的，不会相互分享。另一方面，由于业务有限和缺乏奖励（对某些代理商而言），建立高质量的个人决策政策通常很困难。因此，在私人数据不被泄露的情况下，他们以联邦方式学习决策权是有帮助的。另一个例子是为医院的患者制定医疗治疗政策。患者可能在某些医院接受治疗，从不对治疗提供反馈，这表明这些医院无法根据给予患者的治疗收集奖励，并为患者制定治疗决策政策。此外，有关患者的数据记录是私人的，可能不会在医院之间共享。因此，必须以联邦的方式学习医院的治疗政策。
我们的FRL框架不同于多智能体强化学习，它涉及一组观察全球状态的自动代理（或直接共享以形成“全局”状态的部分状态），选择个体动作和获得团队奖励（Tampuu等，2015; Leibo等，2017）。 FRL假设代理商不分享他们的部分观察结果，而某些代理商无法获得奖励。我们的FRL框架也不同于强化学习中的转移学习，其旨在将学习中获得的经验转移到执行一项任务，以帮助提高相关但不同的任务或代理人的学习成绩，假设观察彼此共享（泰勒） ＆Stone，2009; Tirinzoni等，2018），而FRL假设各州无法在代理商之间共享。
我们的FRL框架分三个阶段运作。最初，每个代理从其他代理收集Q网络的输出值，这些代理用Gausian差分“加密”。此外，它构建神经网络，例如MLP（多层感知器），以计算具有其自己的Q网络输出和加密值作为输入的全局Q网络输出。最后，它根据全球Q网络输出更新MLP及其自己的Q网络。请注意，MLP在代理之间共享，而代理自己的Q网络是其他人不知道的，不应根据训练过程中共享的加密Q网络输出推断出来。
在本文的其余部分，我们首先回顾以前与FRL框架相关的工作，然后介绍FRL的问题表述。之后，我们将详细介绍我们的FRL框架。最后，我们评估了具有各种大小和Text2Actions域的Grid-World域中的FRL框架。


