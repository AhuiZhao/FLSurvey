EXPANDING THE REACH OF FEDERATED LEARNING BY REDUCING CLIENT RESOURCE REQUIREMENTS


Sebastian Caldas∗,a, Jakub Konecˇny`b, H. Brendan McMahanb, Ameet Talwalkara

FL原来的团队发表的

引入了两种新的策略来降低通信成本：（1）在全局模型上使用有损压缩发送服务器到客户端; （2）Federated Dropout，它允许用户在全局模型的较小子集上进行本地高效训练，并且还减少了客户端到服务器的通信和本地计算。

解决的关键问题，通信和边缘计算开销的问题，
相当于有损的发送全局模型压缩，在客户端仅训练小子集（模型蒸馏）
根据实验结果，在有限的压缩程度  dropout程度下，效果确实有损失，但是某些指标上还是更好，相当于一个tradeoff

异构边缘网络上的通信是联邦学习（FL）的一个基本瓶颈，限制了模型容量和用户参与。为了解决这个问题，我们引入了两种新的策略来降低通信成本：（1）在全局模型上使用有损压缩发送服务器到客户端; （2）Federated Dropout，它允许用户在全局模型的较小子集上进行本地高效训练，并且还减少了客户端到服务器的通信和本地计算。我们凭经验表明，这些策略与现有的客户端到服务器通信压缩方法相结合，可以在服务器到客户端通信中共同提供高达14倍的减少，本地计算减少了1.7倍，并且减少了28倍。上传通信，所有这些都不会降低最终模型的质量。因此，我们全面降低了FL对客户端设备资源的影响，允许培训更高容量的模型，以及更多样化的用户群。

