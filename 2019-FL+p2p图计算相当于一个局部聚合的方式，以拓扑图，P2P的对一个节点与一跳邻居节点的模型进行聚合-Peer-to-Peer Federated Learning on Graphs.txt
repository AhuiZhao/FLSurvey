2019----Peer-to-Peer Federated Learning on Graphs

加州大学的工作

相当于一个局部聚合的方式，以拓扑图，P2P的对一个节点与一跳邻居节点的模型进行聚合

摘要
我们考虑在完全分散的框架中通过节点网络训练机器学习模型的问题。 通过引入对模型参数空间的信念，节点采用贝叶斯方法。 我们提出了一种分布式学习算法，其中节点通过将其本地观测数据中的信息与其单跳邻居的模型进行司法汇总来更新其信念，以集体学习最适合整个网络观测的模型。我们的算法推广了联邦学习的先前工作。 此外，我们获得理论保证（上限），网络中的每个节点的误差概率和真实风险都很小。 我们将框架专门用于两个实际相关的线性回归问题和深度神经网络（DNN）的训练。

1.简介
移动计算设备的计算能力和存储容量都在快速增长。在这种增加的计算能力和丰富的数据的帮助下，以及由于隐私和安全性问题，存在越来越多的趋势，即仅使用本地训练数据在这些设备的网络上协作地训练机器学习模型。 McMahan等人发起的联邦学习领域。 （2017年）和Konecny`等。 （2016）考虑了基于多个节点的私人训练数据学习集中模型的问题。更具体地说，该框架的特征在于可能有大量的分散节点，它们（i）连接到中央服务器，（ii）只能访问可能在网络上相关的本地训练数据。还假设节点和中央服务器之间的通信产生大量成本。麦克马汉等人。 （2017）提出了联邦优化算法
中央服务器随机选择每轮中的一小部分节点，与它们共享当前的全局模型，然后平均由所选节点发送回服务器的更新模型。麦克马汉等人。 （2017年）和Konecny`等。 （2016）还使用卷积和递归神经网络提供了具有良好准确性的实验结果，同时降低了通信成本。
这项工作概括了McMahan等人的模型和供给学习框架的框架。 （2017）在以下重要方向。从概念上讲，我们的贡献如下：
•完全分权框架：在所有的训​​练数据是同事lected或集中控制器通过从所有节点聚合信息，以保持对网络的全球模型中，我们并不需要一个集中的位置。相反，在我们的设置中，节点分布在网络/图形上，在那里它们只与它们的单跳邻居通信。因此，我们的问题公式不需要拥有集中控制器。
•本地化数据：我们允许单个节点可用的训练数据不足以学习共享的全局模型。换句话说，节点必须与其下一跳邻居协作以学习最佳模型，即使出于隐私考虑，节点也不与邻居共享其原始训练数据。
为了激励我们的工作并强调我们的贡献，请考虑以下简单的玩具示例。



基于图论提出来的：
节点通过将本地模型信息与其单跳邻居节点的模型进行汇总来更新模型，以集体学习最适合整个网络的模型



